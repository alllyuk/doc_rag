{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Union\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "logger.add(\"../logs/doc_rag.log\", rotation=\"5 MB\", level=\"DEBUG\",\n",
    "           format=\"{time:YYYY-MM-DD at HH:mm:ss} | {level} | {message}\")\n",
    "\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\") # в .env вставить ключ\n",
    "MISTRAL_API_URL = \"https://api.mistral.ai/v1/\"\n",
    "\n",
    "class MistralLLM(LLM):\n",
    "    api_key: str\n",
    "    model_name: str\n",
    "    api_url: str\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mistral\"\n",
    "\n",
    "    def _call(self, system_prompt: str, user_prompt: str,\n",
    "              stop: Optional[List[str]] = None, max_tokens: int = 150, **kwargs) -> str:\n",
    "        client = openai.Client(api_key=self.api_key, base_url=self.api_url)\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            **kwargs\n",
    "        }\n",
    "        logger.debug(\"Request Payload: {}\", payload)\n",
    "        try:\n",
    "            response = client.chat.completions.create(**payload)\n",
    "            logger.debug(\"Response: {}\", response)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error: {}\", e)\n",
    "            raise\n",
    "\n",
    "    def generate(self, system_prompt: str, user_prompt: str, **kwargs) -> str:\n",
    "        return self._call(system_prompt, user_prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "mistral_llm = MistralLLM(api_key=MISTRAL_API_KEY,\n",
    "                         model_name=\"mistral-large-latest\",\n",
    "                         api_url=MISTRAL_API_URL)\n",
    "\n",
    "system_template = \"\"\"You are an assistant that helps with user requests.\n",
    "Be as helpful as possible and return the answer in Russian language.\"\"\"\n",
    "\n",
    "user_template = \"{input_text}\"\n",
    "\n",
    "\n",
    "def generate_response(user_input: str, max_tokens: int = 512) -> str:\n",
    "    system_prompt_template = PromptTemplate(input_variables=[], template=system_template)\n",
    "    user_prompt_template = PromptTemplate(input_variables=[\"input_text\"], template=user_template)\n",
    "\n",
    "    formatted_system_prompt = system_prompt_template.format()\n",
    "    formatted_user_prompt = user_prompt_template.format(input_text=user_input)\n",
    "\n",
    "    response = mistral_llm.generate(system_prompt=formatted_system_prompt,\n",
    "                                    user_prompt=formatted_user_prompt,\n",
    "                                    max_tokens=max_tokens)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 14:00:55.997\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mRequest Payload: {'model': 'mistral-large-latest', 'messages': [{'role': 'system', 'content': 'You are an assistant that helps with user requests.\\nBe as helpful as possible and return the answer in Russian language.'}, {'role': 'user', 'content': 'Посчитай количество калорий в 2 яйцахх'}], 'max_tokens': 512}\u001b[0m\n",
      "\u001b[32m2024-11-23 14:00:59.048\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m41\u001b[0m - \u001b[34m\u001b[1mResponse: ChatCompletion(id='f7d57ed730174e2aa0c395b490e86b59', choices=[Choice(finish_reason='error', index=0, logprobs=None, message=ChatCompletionMessage(content='Количество калорий в яйцах может варьировать в зависимости от их размера, но в среднем одно куриное яйцо среднего размера содержит около 70-80 калорий. Таким образом, два яйца будут содержать примерно 140-160 калорий.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732359656, model='mistral-large-latest', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=93, prompt_tokens=46, total_tokens=139, completion_tokens_details=None, prompt_tokens_details=None))\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Количество калорий в яйцах может варьировать в зависимости от их размера, но '\n",
      " 'в среднем одно куриное яйцо среднего размера содержит около 70-80 калорий. '\n",
      " 'Таким образом, два яйца будут содержать примерно 140-160 калорий.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "user_input = \"Посчитай количество калорий в 2 яйцахх\"\n",
    "response = generate_response(user_input)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralEmbed:\n",
    "    def __init__(self, api_key: str, model_name: str, api_url: str):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.api_url = api_url\n",
    "\n",
    "    @property\n",
    "    def _model_type(self) -> str:\n",
    "        return \"mistral-embed\"\n",
    "\n",
    "    def _call(self, texts: List[str], **kwargs) -> List[List[float]]:\n",
    "        client = openai.Client(api_key=self.api_key, base_url=self.api_url)\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"input\": texts,\n",
    "            **kwargs\n",
    "        }\n",
    "        logger.debug(\"Request Payload: {}\", payload)\n",
    "        try:\n",
    "            response = client.embeddings.create(**payload)\n",
    "            logger.debug(\"Response: {}\", response)\n",
    "            return [embedding.embedding for embedding in response.data]\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error: {}\", e)\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: Union[str, List[str]], **kwargs) -> Union[List[float], List[List[float]]]:\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        return self._call(texts, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing functions\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 1024) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a large text into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        chunk_size (int): The size of each chunk. Default is 1024.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for start in range(0, len(text), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "    return chunks\n",
    "\n",
    "def read_file_and_split_into_chunks(file_path: str, chunk_size: int = 1024) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads a file and splits its content into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the input file.\n",
    "        chunk_size (int): The size of each chunk. Default is 1024.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return split_text_into_chunks(text, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(chunks, batch_size):\n",
    "    return [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_embed = MistralEmbed(api_key=MISTRAL_API_KEY,\n",
    "                             model_name=\"mistral-embed\",\n",
    "                             api_url=MISTRAL_API_URL)\n",
    "\n",
    "\n",
    "file_path = '../data/hmao_npa.txt'\n",
    "chunks = read_file_and_split_into_chunks(file_path)\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "batches = split_into_batches(chunks, batch_size)\n",
    "\n",
    "all_embeddings = []\n",
    "for batch in batches:\n",
    "    embeddings = mistral_embed.generate_embeddings(batch)\n",
    "    all_embeddings.extend(embeddings)\n",
    "\n",
    "for embedding in all_embeddings:\n",
    "    print(len(embedding), embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
