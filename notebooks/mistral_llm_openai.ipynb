{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Union\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "logger.add(\"../logs/doc_rag.log\", rotation=\"5 MB\", level=\"DEBUG\",\n",
    "           format=\"{time:YYYY-MM-DD at HH:mm:ss} | {level} | {message}\")\n",
    "\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\") # в .env вставить ключ\n",
    "MISTRAL_API_URL = \"https://api.mistral.ai/v1/\"\n",
    "\n",
    "class MistralLLM(LLM):\n",
    "    api_key: str\n",
    "    model_name: str\n",
    "    api_url: str\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mistral\"\n",
    "\n",
    "    def _call(self, system_prompt: str, user_prompt: str,\n",
    "              stop: Optional[List[str]] = None, max_tokens: int = 150, **kwargs) -> str:\n",
    "        client = openai.Client(api_key=self.api_key, base_url=self.api_url)\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            **kwargs\n",
    "        }\n",
    "        logger.debug(\"Request Payload: {}\", payload)\n",
    "        try:\n",
    "            response = client.chat.completions.create(**payload)\n",
    "            logger.debug(\"Response: {}\", response)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error: {}\", e)\n",
    "            raise\n",
    "\n",
    "    def generate(self, system_prompt: str, user_prompt: str, **kwargs) -> str:\n",
    "        return self._call(system_prompt, user_prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "mistral_llm = MistralLLM(api_key=MISTRAL_API_KEY,\n",
    "                         model_name=\"mistral-large-latest\",\n",
    "                         api_url=MISTRAL_API_URL)\n",
    "\n",
    "system_template = \"\"\"You are an assistant that helps with user requests.\n",
    "Be as helpful as possible and return the answer in Russian language.\"\"\"\n",
    "\n",
    "user_template = \"{input_text}\"\n",
    "\n",
    "\n",
    "def generate_response(user_input: str, max_tokens: int = 512) -> str:\n",
    "    system_prompt_template = PromptTemplate(input_variables=[], template=system_template)\n",
    "    user_prompt_template = PromptTemplate(input_variables=[\"input_text\"], template=user_template)\n",
    "\n",
    "    formatted_system_prompt = system_prompt_template.format()\n",
    "    formatted_user_prompt = user_prompt_template.format(input_text=user_input)\n",
    "\n",
    "    response = mistral_llm.generate(system_prompt=formatted_system_prompt,\n",
    "                                    user_prompt=formatted_user_prompt,\n",
    "                                    max_tokens=max_tokens)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 14:00:55.997\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mRequest Payload: {'model': 'mistral-large-latest', 'messages': [{'role': 'system', 'content': 'You are an assistant that helps with user requests.\\nBe as helpful as possible and return the answer in Russian language.'}, {'role': 'user', 'content': 'Посчитай количество калорий в 2 яйцахх'}], 'max_tokens': 512}\u001b[0m\n",
      "\u001b[32m2024-11-23 14:00:59.048\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m41\u001b[0m - \u001b[34m\u001b[1mResponse: ChatCompletion(id='f7d57ed730174e2aa0c395b490e86b59', choices=[Choice(finish_reason='error', index=0, logprobs=None, message=ChatCompletionMessage(content='Количество калорий в яйцах может варьировать в зависимости от их размера, но в среднем одно куриное яйцо среднего размера содержит около 70-80 калорий. Таким образом, два яйца будут содержать примерно 140-160 калорий.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732359656, model='mistral-large-latest', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=93, prompt_tokens=46, total_tokens=139, completion_tokens_details=None, prompt_tokens_details=None))\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Количество калорий в яйцах может варьировать в зависимости от их размера, но '\n",
      " 'в среднем одно куриное яйцо среднего размера содержит около 70-80 калорий. '\n",
      " 'Таким образом, два яйца будут содержать примерно 140-160 калорий.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "user_input = \"Посчитай количество калорий в 2 яйцахх\"\n",
    "response = generate_response(user_input)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralEmbed:\n",
    "    def __init__(self, api_key: str, model_name: str, api_url: str):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.api_url = api_url\n",
    "\n",
    "    @property\n",
    "    def _model_type(self) -> str:\n",
    "        return \"mistral-embed\"\n",
    "\n",
    "    def _call(self, texts: List[str], **kwargs) -> List[List[float]]:\n",
    "        client = openai.Client(api_key=self.api_key, base_url=self.api_url)\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"input\": texts,\n",
    "            **kwargs\n",
    "        }\n",
    "        logger.debug(\"Request Payload: {}\", payload)\n",
    "        try:\n",
    "            response = client.embeddings.create(**payload)\n",
    "            logger.debug(\"Response: {}\", response)\n",
    "            return [embedding.embedding for embedding in response.data]\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error: {}\", e)\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: Union[str, List[str]], **kwargs) -> Union[List[float], List[List[float]]]:\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        return self._call(texts, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing functions\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 1024) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a large text into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        chunk_size (int): The size of each chunk. Default is 1024.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for start in range(0, len(text), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "    return chunks\n",
    "\n",
    "def read_file_and_split_into_chunks(file_path: str, chunk_size: int = 1024) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads a file and splits its content into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the input file.\n",
    "        chunk_size (int): The size of each chunk. Default is 1024.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return split_text_into_chunks(text, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(chunks, batch_size):\n",
    "    return [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_embed = MistralEmbed(api_key=MISTRAL_API_KEY,\n",
    "                             model_name=\"mistral-embed\",\n",
    "                             api_url=MISTRAL_API_URL)\n",
    "\n",
    "\n",
    "file_path = '../data/hmao_npa.txt'\n",
    "chunks = read_file_and_split_into_chunks(file_path)\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "batches = split_into_batches(chunks, batch_size)\n",
    "\n",
    "all_embeddings = []\n",
    "for batch in batches:\n",
    "    embeddings = mistral_embed.generate_embeddings(batch)\n",
    "    all_embeddings.extend(embeddings)\n",
    "\n",
    "for embedding in all_embeddings:\n",
    "    print(len(embedding), embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_embed.generate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([batch for batch in batches[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [get_token_count_embedding(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches[:\u001b[38;5;241m10\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[156], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[43mget_token_count_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches[:\u001b[38;5;241m10\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m, in \u001b[0;36mget_token_count_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_count_embedding\u001b[39m(text):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstruct_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_user_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/ML/ML_env/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/sentencepiece.py:293\u001b[0m, in \u001b[0;36mInstructTokenizerV1.encode_user_content\u001b[0;34m(self, content, is_last, system_prompt, force_img_first)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_user_content\u001b[39m(\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    288\u001b[0m     content: Union[\u001b[38;5;28mstr\u001b[39m, List[ContentChunk]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     force_img_first: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mint\u001b[39m], List[np\u001b[38;5;241m.\u001b[39mndarray]]:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_last \u001b[38;5;129;01mand\u001b[39;00m system_prompt:\n\u001b[1;32m    296\u001b[0m         content \u001b[38;5;241m=\u001b[39m system_prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m content\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[get_token_count_embedding(batch) for batch in batches[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "MilvusException",
     "evalue": "<MilvusException: (code=2, message=Fail connecting to server on localhost:9091, illegal connection params or server unavailable)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFutureTimeoutError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/ML/ML_env/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py:147\u001b[0m, in \u001b[0;36mGrpcHandler._wait_for_channel_ready\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mgrpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_ready_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_channel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_identifier_interceptor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/ML/ML_env/lib/python3.10/site-packages/grpc/_utilities.py:162\u001b[0m, in \u001b[0;36m_ChannelReadyFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ML/ML_env/lib/python3.10/site-packages/grpc/_utilities.py:106\u001b[0m, in \u001b[0;36m_ChannelReadyFuture._block\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mFutureTimeoutError()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFutureTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMilvusException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymilvus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connections\n\u001b[0;32m----> 3\u001b[0m \u001b[43mconnections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m9091\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ML/ML_env/lib/python3.10/site-packages/pymilvus/orm/connections.py:449\u001b[0m, in \u001b[0;36mConnections.connect\u001b[0;34m(self, alias, user, password, db_name, token, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_uri\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    447\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecure\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[43mconnect_milvus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# 2nd Priority, connection configs from env\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/ML_env/lib/python3.10/site-packages/pymilvus/orm/connections.py:400\u001b[0m, in \u001b[0;36mConnections.connect.<locals>.connect_milvus\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m t \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    398\u001b[0m timeout \u001b[38;5;241m=\u001b[39m t \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m Config\u001b[38;5;241m.\u001b[39mMILVUS_CONN_TIMEOUT\n\u001b[0;32m--> 400\u001b[0m \u001b[43mgh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_channel_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    402\u001b[0m     gh\u001b[38;5;241m.\u001b[39mregister_state_change_callback(\n\u001b[1;32m    403\u001b[0m         ReconnectHandler(\u001b[38;5;28mself\u001b[39m, alias, kwargs_copy)\u001b[38;5;241m.\u001b[39mreconnect_on_idle\n\u001b[1;32m    404\u001b[0m     )\n",
      "File \u001b[0;32m~/ML/ML_env/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py:150\u001b[0m, in \u001b[0;36mGrpcHandler._wait_for_channel_ready\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_identifier_interceptor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mFutureTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MilvusException(\n\u001b[1;32m    151\u001b[0m         code\u001b[38;5;241m=\u001b[39mStatus\u001b[38;5;241m.\u001b[39mCONNECT_FAILED,\n\u001b[1;32m    152\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail connecting to server on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_address\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, illegal connection params or server unavailable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mMilvusException\u001b[0m: <MilvusException: (code=2, message=Fail connecting to server on localhost:9091, illegal connection params or server unavailable)>"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "\n",
    "tokenizer_embed = MistralTokenizer.v1()\n",
    "\n",
    "\n",
    "def get_token_count_embedding(text: str) -> int:\n",
    "    return len(tokenizer_embed.instruct_tokenizer.encode_user_content(\n",
    "        text, is_last=True)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_count_embedding('12321321321')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([28705,\n",
       "  28740,\n",
       "  28750,\n",
       "  28770,\n",
       "  28750,\n",
       "  28740,\n",
       "  28770,\n",
       "  28750,\n",
       "  28740,\n",
       "  28770,\n",
       "  28750,\n",
       "  28740],\n",
       " [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_embed.instruct_tokenizer.encode_user_content('12321321321', is_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InstructTokenizerV1' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstruct_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m32131\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InstructTokenizerV1' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "tokenizer_embed.instruct_tokenizer.encode('32131')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
