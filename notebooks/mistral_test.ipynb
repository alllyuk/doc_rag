{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "class MistralLLM(LLM):\n",
    "    api_key: str\n",
    "    model_name: str\n",
    "    api_url: str = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mistral\"\n",
    "\n",
    "    def _call(self, system_prompt: str, user_prompt: str,\n",
    "              stop: Optional[List[str]] = None, max_tokens: int = 150, **kwargs) -> str:\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            # \"temperature\": kwargs.get(\"temperature\", 1),\n",
    "            # \"top_p\": kwargs.get(\"top_p\", 1),\n",
    "            # \"stream\": kwargs.get(\"stream\", False),\n",
    "            # \"stop\": stop or [\"string\"],\n",
    "            # \"random_seed\": kwargs.get(\"random_seed\", 0),\n",
    "            # \"response_format\": {\"type\": \"text\"},\n",
    "            # \"tools\": kwargs.get(\"tools\", []),\n",
    "            # \"tool_choice\": kwargs.get(\"tool_choice\", \"auto\"),\n",
    "            # \"presence_penalty\": kwargs.get(\"presence_penalty\", 0),\n",
    "            # \"frequency_penalty\": kwargs.get(\"frequency_penalty\", 0),\n",
    "            # \"n\": kwargs.get(\"n\", 1),\n",
    "            # \"safe_prompt\": kwargs.get(\"safe_prompt\", False)\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "        response_data = response.json()\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Mistral API Error: {response_data.get('message', 'Unknown error')}\")\n",
    "\n",
    "        return response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "    def generate(self, system_prompt: str, user_prompt: str, **kwargs) -> str:\n",
    "        return self._call(system_prompt, user_prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конечно! Вот идея для мема про Штирлица и машинное обучение:\n",
      "\n",
      "**Изображение:** Штирлиц сидит за столом, окружённый кучей бумаг и книг, на экране компьютера виден график обучения модели.\n",
      "\n",
      "**Текст:**\n",
      "\n",
      "**Верхний текст:** \"Когда Штирлиц пытается объяснить Мюллеру, почему его модель машинного обучения не может предсказать поведение Гитлера.\"\n",
      "\n",
      "**Нижний текст:** \"Мюллер: 'Штирлиц, вы уверены, что у вас достаточно данных?'\"\n",
      "\n",
      "**Подпись:** \"Штирлиц: 'Данные есть, но Гитлер — это такой аутлаер, что модель просто не справляется.'\"\n",
      "\n",
      "Надеюсь, вам понравится этот мем!\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "mistral_llm = MistralLLM(api_key=MISTRAL_API_KEY,\n",
    "                         model_name=\"mistral-large-latest\")\n",
    "\n",
    "\n",
    "system_template = \"You are an assistant that helps with translations.\"\n",
    "user_template = \"{input_text}\"\n",
    "\n",
    "system_prompt_template = PromptTemplate(input_variables=[], template=system_template)\n",
    "user_prompt_template = PromptTemplate(input_variables=[\"input_text\"], template=user_template)\n",
    "\n",
    "user_input = \"Придумай мем про Штирлица и машинное обучение\"\n",
    "\n",
    "formatted_system_prompt = system_prompt_template.format()\n",
    "formatted_user_prompt = user_prompt_template.format(input_text=user_input)\n",
    "\n",
    "\n",
    "response = mistral_llm.generate(system_prompt=formatted_system_prompt,\n",
    "                                user_prompt=formatted_user_prompt,\n",
    "                                max_tokens=512)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
